import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import NMF
import numpy as np

# íŒŒì¼ ë¡œë“œ (BERT ì˜ˆì¸¡ ê²°ê³¼ íŒŒì¼)
OUTPUT_PREDICTED_FILE = "FINAL_ANALYSIS_DATA_with_Sentiment.csv"
try:
    # íŒŒì¼ì„ ë‹¤ì‹œ ë¡œë“œí•˜ì—¬ ë¶€ì •ì  ë°ì´í„°ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
    df_final = pd.read_csv(OUTPUT_PREDICTED_FILE)
except FileNotFoundError:
    print(f"[ERROR] ìµœì¢… ì˜ˆì¸¡ íŒŒì¼ ('{OUTPUT_PREDICTED_FILE}')ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¶„ì„ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.")
    exit()

# 1. ë¶€ì •ì ì¸ í…ìŠ¤íŠ¸ë§Œ í•„í„°ë§ (ë…¼ë€ì˜ í•µì‹¬)
df_negative = df_final[df_final['Predicted_Sentiment'] == 'Negative'].copy()
docs = df_negative['text'].astype(str).tolist()

# 2. ë¶ˆìš©ì–´ ë° í‚¤ì›Œë“œ ì„¤ì • (ì´ì „ ë‹¨ê³„ì—ì„œ ì‚¬ìš©ëœ ëª©ë¡ ì¬ì‚¬ìš©)
STOP_WORDS_LIST = [
    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',
    'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its',
    'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom',
    'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
    'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but',
    'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',
    'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below',
    'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',
    'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each',
    'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same',
    'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now',

    # í”„ë¡œì íŠ¸ ê´€ë ¨ ë¶ˆí•„ìš”í•œ ê³µí†µ í‚¤ì›Œë“œ ë° ì¡ìŒ ì¶”ê°€
    'covid', 'vaccine', 'get', 'would', 'could', 'one', 'take', 'need', 'people', 'us', 'say',
    'make', 'go', 'know', 'see', 'many', 'like', 'think', 'dont', 'im', 'ive', 'said', 'thats',
    'really', 'back', 'much', 'still', 'even', 'want', 'time', 'also', 'something', 'going',
    'look', 'lot', 'way', 'got', 'didnt', 'anyone', 'new', 'ever', 'may', 'tell', 'last',
    'week', 'every', 'things', 'using', 'way', 'since', 'first', 'getting', 'without'
]

# 3. CountVectorizer ì„¤ì • ë° ì ìš© (ë‹¨ì–´ ë¹ˆë„ í–‰ë ¬ ìƒì„±)
# min_dfë¥¼ 50ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë…¸ì´ì¦ˆ ë° í¬ê·€ ë‹¨ì–´ ì œê±°
vectorizer = CountVectorizer(
    stop_words=STOP_WORDS_LIST,
    min_df=50,
    ngram_range=(1, 2)  # ë‹¨ì–´ 1ê°œ ë˜ëŠ” 2ê°œ ì¡°í•©(ë¹…ê·¸ë¨) ì‚¬ìš©
)
dtm = vectorizer.fit_transform(docs)

# 4. NMF ëª¨ë¸ í•™ìŠµ (5ê°œ í† í”½ ì¶”ì¶œ)
num_topics = 5
# NMFëŠ” í† í”½ ëª¨ë¸ë§ì— íš¨ê³¼ì ì´ë©°, max_iterë¥¼ 300ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì•ˆì •ì ì¸ ê²°ê³¼ë¥¼ ìœ ë„
nmf = NMF(n_components=num_topics, random_state=1, max_iter=300)
nmf.fit(dtm)

feature_names = vectorizer.get_feature_names_out()
topic_results = []
top_words_count = 10

# 5. í† í”½ë³„ ìƒìœ„ ë‹¨ì–´ ì¶”ì¶œ ë° ì €ì¥
for topic_idx, topic in enumerate(nmf.components_):
    top_features_ind = topic.argsort()[:-top_words_count - 1:-1]
    top_features = [feature_names[i] for i in top_features_ind]
    topic_results.append({
        'Topic': f'í† í”½ {topic_idx + 1}',
        'Keywords': ', '.join(top_features)
    })

# 6. ìµœì¢… ì¶œë ¥
df_topics = pd.DataFrame(topic_results)

print("\n## ğŸ—ºï¸ NMF ê¸°ë°˜ ë¶€ì • ì—¬ë¡  í•µì‹¬ í† í”½ ì¶”ì¶œ")
print("---")
print(f"ë¶„ì„ ëŒ€ìƒ ë°ì´í„°: {len(df_negative)}ê±´ (Negative Sentiment)")
print(f"ì¶”ì¶œëœ í† í”½ ìˆ˜: {num_topics}ê°œ")
print("\n--- í† í”½ë³„ ìƒìœ„ í‚¤ì›Œë“œ ---")
print(df_topics.to_markdown(index=False))