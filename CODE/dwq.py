import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
# === ëª¨ë¸ ë³€ê²½: Electra ëŒ€ì‹  Bert ì‚¬ìš© ===
from transformers import BertTokenizer, BertForSequenceClassification
from torch.optim import AdamW
from tqdm import tqdm
from collections import Counter

# 1ï¸âƒ£ GPU ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)


# 2ï¸âƒ£ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜ (ìƒëµ, ì´ì „ê³¼ ë™ì¼)
class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        inputs = self.tokenizer(
            text, return_tensors='pt', max_length=self.max_len, padding='max_length', truncation=True
        )
        item = {key: val.squeeze(0) for key, val in inputs.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item


# 3ï¸âƒ£ í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜
def train_and_evaluate(csv_path, num_labels, output_pred_path):
    # === BertForSequenceClassification í•¨ìˆ˜ì— ë§ê²Œ ëª¨ë¸ ì´ë¦„ ë° í´ë˜ìŠ¤ ìˆ˜ì • ===
    MODEL_NAME = "bert-base-multilingual-cased"

    print(f"\n===== {csv_path} ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ëª¨ë¸: BERT, í´ë˜ìŠ¤ ìˆ˜: {num_labels}) =====")

    # ë°ì´í„° ë¡œë“œ (ì´ì „ê³¼ ë™ì¼)
    df = pd.read_csv(csv_path)

    if "binary" in csv_path.lower():
        label_col = "sentiment_binary";
        label_map = {"ë¶€ì •": 0, "ê¸ì •": 1}
        custom_lr = 2e-5;
        adjust_weights_manually = False

    elif "three" in csv_path.lower():
        label_col = "sentiment_three";
        label_map = {"ë¶€ì •": 0, "ì¤‘ë¦½": 1, "ê¸ì •": 2}
        custom_lr = 1e-5;
        adjust_weights_manually = True

    else:
        raise ValueError("CSV íŒŒì¼ ì´ë¦„ì— 'binary' ë˜ëŠ” 'three'ê°€ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")

    if label_col not in df.columns:
        raise ValueError(f"'{label_col}' ì»¬ëŸ¼ì´ CSVì— ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ì»¬ëŸ¼: {df.columns.tolist()}")

    texts = df['text'].astype(str).tolist()
    labels = [label_map[l] for l in df[label_col].tolist()]

    # 3.1. í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ (ìƒëµ)
    train_texts, val_texts, train_labels, val_labels = train_test_split(
        texts, labels, test_size=0.2, random_state=42, stratify=labels
    )
    print(f"Original Training Distribution: {Counter(train_labels)}")

    # 3.2. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ë° ì ìš©
    class_counts = Counter(train_labels)
    total_samples = len(train_labels)

    if adjust_weights_manually:
        # ğŸ§ª [Three-Class] ìˆ˜ë™ ê°€ì¤‘ì¹˜ ì¡°ì • ì „ëµ: 'ì¤‘ë¦½'ì— ìµœëŒ€ í˜ë„í‹° ë¶€ì—¬
        weights = [
            0.50,  # ë¶€ì •
            3.50,  # ì¤‘ë¦½ (ìµœëŒ€ í˜ë„í‹°)
            1.50  # ê¸ì •
        ]
        weights_tensor = torch.tensor(weights, dtype=torch.float).to(device)
        print(f"Calculated Custom Class Weights (Order 0, 1, 2): {weights_tensor}")

    else:
        # [Binary] ìë™ ì—­ë¹ˆë„ ê°€ì¤‘ì¹˜ ì‚¬ìš©
        class_weights = {i: total_samples / (num_labels * count) for i, count in class_counts.items()}
        weights_tensor = torch.tensor([class_weights[i] for i in sorted(class_weights.keys())], dtype=torch.float).to(
            device)
        print(f"Calculated Auto Class Weights (Order 0, 1): {weights_tensor}")

    # 3.3. í† í¬ë‚˜ì´ì € ë° ë°ì´í„°ì…‹
    # === BERT í† í¬ë‚˜ì´ì €ë¡œ ë³€ê²½ ===
    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)
    train_dataset = TextDataset(train_texts, train_labels, tokenizer)
    val_dataset = TextDataset(val_texts, val_labels, tokenizer)

    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32)

    # 3.4. ëª¨ë¸ ë° ì†ì‹¤ í•¨ìˆ˜
    # === BertForSequenceClassificationìœ¼ë¡œ ë³€ê²½ ===
    model = BertForSequenceClassification.from_pretrained(
        MODEL_NAME,
        num_labels=num_labels
        # BertForSequenceClassificationì€ 'use_safetensors' ì¸ìˆ˜ë¥¼ ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì œê±°
    ).to(device)

    # === í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ë¥¼ Loss Functionì— ì ìš© ===
    loss_fn = torch.nn.CrossEntropyLoss(weight=weights_tensor)

    optimizer = AdamW(model.parameters(), lr=custom_lr)
    epochs = 8

    # 3.5. í•™ìŠµ, 3.6. ê²€ì¦, 3.7. ì˜ˆì¸¡ (ì´ì „ê³¼ ë™ì¼)
    # ... (í•™ìŠµ, ê²€ì¦, ì˜ˆì¸¡ ë£¨í”„ëŠ” ì´ì „ê³¼ ë™ì¼) ...
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch in tqdm(train_loader, desc=f"Epoch {epoch + 1}"):
            optimizer.zero_grad()
            inputs = {k: v.to(device) for k, v in batch.items()}
            # outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])
            # outputsì˜ í‚¤ê°€ Bert ëª¨ë¸ê³¼ Electra ëª¨ë¸ ê°„ì— ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì¸ìë¥¼ ëª…ì‹œ
            outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])
            loss = loss_fn(outputs.logits, inputs['labels'])

            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch + 1} | Train Loss: {total_loss / len(train_loader):.4f}")

    # 3.6. ê²€ì¦ (ìƒëµ)
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for batch in val_loader:
            inputs = {k: v.to(device) for k, v in batch.items()}
            outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])
            preds = torch.argmax(outputs.logits, dim=1)
            correct += (preds.cpu() == inputs['labels'].cpu()).sum().item()
            total += len(preds)
    accuracy = correct / total
    print(f"âœ… Validation Accuracy: {accuracy:.4f}")

    # 3.7. ì›ë³¸ ë°ì´í„° ì˜ˆì¸¡ (ìƒëµ)
    final_df = pd.read_csv("FINAL_DATA_FILTERED_#TRUE.csv")
    final_texts = final_df['text'].astype(str).tolist()
    final_dataset = TextDataset(final_texts, [0] * len(final_texts), tokenizer)
    final_loader = DataLoader(final_dataset, batch_size=32)

    model.eval()
    preds_list = []
    with torch.no_grad():
        for batch in tqdm(final_loader, desc="Predicting FINAL dataset"):
            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}
            outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])
            preds = torch.argmax(outputs.logits, dim=1)
            preds_list.extend(preds.cpu().numpy())

    # ê²°ê³¼ ì €ì¥ (íŒŒì¼ëª…ì— BERT ëª…ì‹œ)
    final_df['predicted_label'] = preds_list
    final_df.to_csv(output_pred_path.replace("koelectra", "bert"), index=False)
    print(f"ğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_pred_path.replace('koelectra', 'bert')}")
    print("========================================\n")

    return accuracy


# 4ï¸âƒ£ ì´ì§„ ë¶„ë¥˜ (binary)
binary_acc = train_and_evaluate(
    "BERT_labeled_binary.csv",
    num_labels=2,
    output_pred_path="predicted_binary_lr_epochs_tuned_final_bert.csv"
)

# 5ï¸âƒ£ ì‚¼ë¶„ë¥˜ (three-class)
three_acc = train_and_evaluate(
    "BERT_labeled_three.csv",
    num_labels=3,
    output_pred_path="predicted_three_lr_epochs_tuned_final_bert.csv"
)

print("ğŸ¯ ìµœì¢… ê²°ê³¼")
print(f"Binary Validation Accuracy : {binary_acc:.4f}")
print(f"Three-class Validation Accuracy : {three_acc:.4f}")