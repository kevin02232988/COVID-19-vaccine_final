import pandas as pd
from bertopic import BERTopic
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
import nltk
import os

# -------------------------------------------------------------------------------------
# 0. í™˜ê²½ ì„¤ì • ë° ì •ì˜
# -------------------------------------------------------------------------------------
# NLTK ë° ë¶ˆìš©ì–´ ì„¤ì •
try:
    # stopwordsê°€ ë‹¤ìš´ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    nltk.data.find('corpora/stopwords')
except nltk.downloader.DownloadError:
    nltk.download('stopwords')
stop_words = stopwords.words('english')
vectorizer_model = CountVectorizer(stop_words=stop_words)

# -------------------------------------------------------------------------------------
# 1. ëª¨ë¸ ë¡œë“œ ë° ë°ì´í„° ì¤€ë¹„
# -------------------------------------------------------------------------------------

topic_model_neg = None
topic_model_pos = None

# 1-1. ëª¨ë¸ ë¡œë“œ ì‹œë„
try:
    # ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•œë‹¤ê³  ê°€ì •í•˜ê³  ë¡œë“œ
    topic_model_neg = BERTopic.load("bertopic_negative")
    topic_model_pos = BERTopic.load("bertopic_positive")
    print("âœ… BERTopic ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (bertopic_negative, bertopic_positive).")

except Exception as e:
    print(f"âš ï¸ ê²½ê³ : BERTopic ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ({e}). í† í”½ ê²°ê³¼ë¥¼ ë³´ê¸° ìœ„í•´ ëª¨ë¸ì„ ë‹¤ì‹œ í•™ìŠµí•©ë‹ˆë‹¤.")

    # 1-2. ë¡œë“œ ì‹¤íŒ¨ ì‹œ, ë°ì´í„° ë¡œë“œ ë° ì¬í•™ìŠµì„ í†µí•´ ê²°ê³¼ ìƒì„±
    try:
        # ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ CSV íŒŒì¼ ë¡œë“œ ë° ë³‘í•© (ì¬í•™ìŠµì„ ìœ„í•œ í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„)
        df_neg_loaded = pd.read_csv("negative_topics.csv")
        df_pos_loaded = pd.read_csv("positive_topics.csv")
        df = pd.concat([df_neg_loaded, df_pos_loaded], ignore_index=True)

        df = df.dropna(subset=['text', 'sentiment_label'])
        df['sentiment_label'] = df['sentiment_label'].astype(int)

        df_neg = df[df['sentiment_label'] == 0].copy()
        df_pos = df[df['sentiment_label'] == 1].copy()

        # ë¶€ì • ë¦¬ë·° ì¬í•™ìŠµ (ê²°ê³¼ ì¶œë ¥ì´ ëª©ì ì´ë¯€ë¡œ í™•ë¥  ê³„ì‚°ì€ ìƒëµ)
        print("\nğŸ”µ ë¶€ì • ë¦¬ë·° í† í”½ ëª¨ë¸ ì¬í•™ìŠµ ì‹œì‘...")
        topic_model_neg = BERTopic(
            vectorizer_model=vectorizer_model,
            language="multilingual",
            calculate_probabilities=False
        ).fit(df_neg["text"])
        topic_model_neg.save("bertopic_negative")  # ë‹¤ìŒ ì‹¤í–‰ì„ ìœ„í•´ ì €ì¥

        # ê¸ì • ë¦¬ë·° ì¬í•™ìŠµ
        print("\nğŸŸ¢ ê¸ì • ë¦¬ë·° í† í”½ ëª¨ë¸ ì¬í•™ìŠµ ì‹œì‘...")
        topic_model_pos = BERTopic(
            vectorizer_model=vectorizer_model,
            language="multilingual",
            calculate_probabilities=False
        ).fit(df_pos["text"])
        topic_model_pos.save("bertopic_positive")  # ë‹¤ìŒ ì‹¤í–‰ì„ ìœ„í•´ ì €ì¥

        print("\nâœ… BERTopic ëª¨ë¸ ì¬í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ.")

    except FileNotFoundError:
        print("\nâŒ ì˜¤ë¥˜: ëª¨ë¸ ë¡œë“œ ë° ì¬í•™ìŠµ ëª¨ë‘ ì‹¤íŒ¨. ì›ë³¸ CSV íŒŒì¼(negative_topics.csv, positive_topics.csv)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        exit()
    except Exception as ee:
        print(f"\nâŒ ì‹¬ê°í•œ ì˜¤ë¥˜: ì¬í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({ee})")
        exit()


# -------------------------------------------------------------------------------------
# 2. í† í”½ ì •ë³´ ì¶”ì¶œ ë° ì¶œë ¥ (í•µì‹¬ ê²°ê³¼)
# -------------------------------------------------------------------------------------

def print_topic_summary(model, title):
    """ëª¨ë¸ì˜ í† í”½ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  í¬ë§·í•˜ì—¬ ì¶œë ¥í•©ë‹ˆë‹¤."""
    topic_info = model.get_topic_info()

    # ë…¸ì´ì¦ˆ í† í”½(-1) ì œì™¸í•˜ê³  ì˜ë¯¸ ìˆëŠ” í† í”½ë§Œ ì„ íƒ
    meaningful_topics = topic_info[topic_info['Topic'] != -1].copy()

    print("\n" + "=" * 80)
    print(f"## {title} - ì˜ë¯¸ ìˆëŠ” ì£¼ìš” í† í”½ (ìƒìœ„ 10ê°œ)")
    print("=" * 80)

    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
    display_cols = ['Topic', 'Count', 'Name', 'Representation']
    display_df = meaningful_topics[display_cols]

    # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ í‚¤ì›Œë“œë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥ ê°€ë…ì„± ë†’ì´ê¸°
    display_df['Representation'] = display_df['Representation'].apply(lambda x: ', '.join(x))

    # Markdown í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
    print(display_df.head(10).to_markdown(index=False))

    # ì „ì²´ ë¬¸ì„œ ìˆ˜ ìš”ì•½
    total_docs = topic_info['Count'].sum()
    noise_docs = topic_info[topic_info['Topic'] == -1]['Count'].iloc[0] if -1 in topic_info['Topic'].values else 0
    print("-" * 80)
    print(f"ì´ ë¶„ì„ ë¬¸ì„œ ìˆ˜: {total_docs} | ë…¸ì´ì¦ˆ(-1) í† í”½ ë¬¸ì„œ ìˆ˜: {noise_docs} | ì˜ë¯¸ ìˆëŠ” í† í”½ ë¬¸ì„œ ìˆ˜: {total_docs - noise_docs}")
    print("=" * 80)


if topic_model_neg:
    print_topic_summary(topic_model_neg, "ğŸ“‰ ë¶€ì • ë¦¬ë·° í† í”½ ëª¨ë¸ ê²°ê³¼")

if topic_model_pos:
    print_topic_summary(topic_model_pos, "ğŸ“ˆ ê¸ì • ë¦¬ë·° í† í”½ ëª¨ë¸ ê²°ê³¼")

print("\n\nğŸ‰ ì½”ë“œë¥¼ ëŒë¦° ê²°ê³¼(ì˜ë¯¸ìˆëŠ” í† í”½ í‚¤ì›Œë“œ) ì¶œë ¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (ì¶”ê°€ ì‹œê°í™” ê¸°ëŠ¥ì€ ëª¨ë‘ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.)")