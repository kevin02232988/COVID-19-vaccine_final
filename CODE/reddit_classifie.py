import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
import torch
from torch.utils.data import Dataset
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# ------------------- íŒŒì¼ ë° ì„¤ì • ì •ì˜ -------------------
# í•™ìŠµ ë°ì´í„° ê²½ë¡œ: ë³µêµ¬ëœ íŒŒì¼ë¡œ ë³€ê²½
FILE_USER_LABELED = "Real_rabel_labeled_CLEANED.csv"
FILE_FULL_DATA = "Real_Final.csv" # ì „ì²´ 98,277ê±´ ë°ì´í„°
OUTPUT_PREDICTED_FILE = "FINAL_ANALYSIS_DATA_with_Sentiment.csv"

# ------------------- 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ -------------------

# 1-1. í•™ìŠµ ë°ì´í„° (9,827ê±´) ë¡œë“œ
df_train = pd.read_csv(FILE_USER_LABELED).fillna('')
df_train = df_train[df_train['sentiment'].isin(['positive', 'negative'])].copy() # ì†Œë¬¸ìë¡œ ì •ê·œí™”ëœ ë¼ë²¨ ì‚¬ìš©

# 1-2. ë¼ë²¨ ë§¤í•‘ (Negative: 0, Positive: 1)
label_map = {'negative': 0, 'positive': 1}
df_train['labels'] = df_train['sentiment'].map(label_map)

# 1-3. ì „ì²´ ì˜ˆì¸¡ ëŒ€ìƒ ë°ì´í„° (98,277ê±´) ë¡œë“œ
df_predict = pd.read_csv(FILE_FULL_DATA).fillna('')

print(f"[INFO] í•™ìŠµ ë°ì´í„°ì…‹ í¬ê¸°: {len(df_train)}ê±´")
print(f"[INFO] ì „ì²´ ì˜ˆì¸¡ ëŒ€ìƒ ë°ì´í„° í¬ê¸°: {len(df_predict)}ê±´")

# ------------------- 2. Dataset ë° Tokenizer ì¤€ë¹„ -------------------
MODEL_NAME = 'bert-base-multilingual-cased'
tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)

class SentimentDataset(Dataset):
    def __init__(self, encodings, labels=None):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        if self.labels is not None:
            item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.encodings.input_ids)

# ë°ì´í„° ë¶„ë¦¬ (í•™ìŠµ: 80%, ê²€ì¦: 20%)
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df_train['text'].tolist(),
    df_train['labels'].tolist(),
    test_size=0.2,
    random_state=42,
    stratify=df_train['labels']
)

# ì¸ì½”ë”©
train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)
val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)

train_dataset = SentimentDataset(train_encodings, train_labels)
val_dataset = SentimentDataset(val_encodings, val_labels)

# ------------------- 3. ëª¨ë¸ í•™ìŠµ (Fine-tuning) -------------------
model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)

def compute_metrics(p):
    preds = np.argmax(p.predictions, axis=1)
    # pos_label=1ì€ Positive (1)ì„ ê¸ì • í´ë˜ìŠ¤ë¡œ ê°„ì£¼í•¨ì„ ì˜ë¯¸
    precision, recall, f1, _ = precision_recall_scores = precision_recall_fscore_support(p.label_ids, preds, average='binary', pos_label=1)
    acc = accuracy_score(p.label_ids, preds)
    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}

# í•™ìŠµ ì¸ì ì„¤ì • (ìˆ˜ì •ëœ ë¶€ë¶„: evaluation_strategy ì œê±° ë° eval_strategy ì‚¬ìš©)
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=100,
    # evaluation_strategy ëŒ€ì‹  eval_strategy ì‚¬ìš©
    eval_strategy="epoch",
    save_strategy="epoch",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
)

print("\n--- 3. BERT ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ì•½ 9,827ê±´) ---")
trainer.train()

# ------------------- 4. ëª¨ë¸ ì˜ˆì¸¡ ë° ìµœì¢… ì €ì¥ -------------------

# 4-1. ì˜ˆì¸¡ ëŒ€ìƒ ë°ì´í„°ì…‹ ì¤€ë¹„
df_predict['text'] = df_predict['text'].astype(str).tolist()
predict_texts = df_predict['text'].tolist()
predict_encodings = tokenizer(predict_texts, truncation=True, padding=True, max_length=128)
predict_dataset = SentimentDataset(predict_encodings)

# 4-2. ì˜ˆì¸¡ ì‹¤í–‰
print("\n--- 4. ì „ì²´ ë°ì´í„°ì…‹ ê°ì • ì˜ˆì¸¡ ì‹œì‘ (98,277ê±´) ---")
predictions = trainer.predict(predict_dataset)
predicted_labels = np.argmax(predictions.predictions, axis=1)

# 4-3. ë¼ë²¨ ë””ì½”ë”© ë° ì €ì¥
# 0 -> Negative, 1 -> Positive
label_decode = {0: 'Negative', 1: 'Positive'}
df_predict['Predicted_Sentiment'] = [label_decode[label] for label in predicted_labels]

# 4-4. ìµœì¢… CSV ì €ì¥
df_predict.to_csv(OUTPUT_PREDICTED_FILE, index=False, encoding="utf-8-sig")

print(f"\nâœ… ìµœì¢… ë¶„ì„ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ! ì´ {len(df_predict)}ê±´")
print(f"ğŸ’¾ íŒŒì¼ ì €ì¥ ì™„ë£Œ: '{OUTPUT_PREDICTED_FILE}'")