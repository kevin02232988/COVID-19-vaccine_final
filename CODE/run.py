import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from transformers import ElectraTokenizer, ElectraForSequenceClassification
from torch.optim import AdamW
from tqdm import tqdm

# 1ï¸âƒ£ GPU ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# 2ï¸âƒ£ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        inputs = self.tokenizer(
            text, return_tensors='pt', max_length=self.max_len, padding='max_length', truncation=True
        )
        item = {key: val.squeeze(0) for key, val in inputs.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item

# 3ï¸âƒ£ í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜
def train_and_evaluate(csv_path, num_labels, output_pred_path):
    print(f"\n===== {csv_path} ëª¨ë¸ í•™ìŠµ ì‹œì‘ =====")

    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(csv_path)

    # CSV ì´ë¦„ì— ë”°ë¼ ë¼ë²¨ ì»¬ëŸ¼ê³¼ ë§¤í•‘ ì„ íƒ
    if "binary" in csv_path.lower():
        label_col = "sentiment_binary"
        label_map = {"ë¶€ì •": 0, "ê¸ì •": 1}
    elif "three" in csv_path.lower():
        label_col = "sentiment_three"
        label_map = {"ë¶€ì •": 0, "ì¤‘ë¦½": 1, "ê¸ì •": 2}
    else:
        raise ValueError("CSV íŒŒì¼ ì´ë¦„ì— 'binary' ë˜ëŠ” 'three'ê°€ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")

    if label_col not in df.columns:
        raise ValueError(f"'{label_col}' ì»¬ëŸ¼ì´ CSVì— ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ì»¬ëŸ¼: {df.columns.tolist()}")

    texts = df['text'].astype(str).tolist()
    # ë¬¸ìì—´ ë¼ë²¨ì„ ìˆ«ìë¡œ ë³€í™˜
    labels = [label_map[l] for l in df[label_col].tolist()]

    # ë°ì´í„° ë¶„ë¦¬
    train_texts, val_texts, train_labels, val_labels = train_test_split(
        texts, labels, test_size=0.2, random_state=42, stratify=labels
    )

    # í† í¬ë‚˜ì´ì € ë° ë°ì´í„°ì…‹
    tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-base-v3-discriminator")
    train_dataset = TextDataset(train_texts, train_labels, tokenizer)
    val_dataset = TextDataset(val_texts, val_labels, tokenizer)

    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32)

    # ëª¨ë¸
    model = ElectraForSequenceClassification.from_pretrained(
        "monologg/koelectra-base-v3-discriminator",
        use_safetensors=True
    ).to(device)

    optimizer = AdamW(model.parameters(), lr=2e-5)
    epochs = 2

    # í•™ìŠµ
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}"):
            optimizer.zero_grad()
            inputs = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**inputs)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch+1} | Train Loss: {total_loss / len(train_loader):.4f}")

    # ê²€ì¦
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for batch in val_loader:
            inputs = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**inputs)
            preds = torch.argmax(outputs.logits, dim=1)
            correct += (preds.cpu() == inputs['labels'].cpu()).sum().item()
            total += len(preds)
    accuracy = correct / total
    print(f"âœ… Validation Accuracy: {accuracy:.4f}")

    # ì›ë³¸ ë°ì´í„° ì˜ˆì¸¡
    final_df = pd.read_csv("FINAL_DATA_FILTERED_#TRUE.csv")
    final_texts = final_df['text'].astype(str).tolist()

    final_dataset = TextDataset(final_texts, [0]*len(final_texts), tokenizer)
    final_loader = DataLoader(final_dataset, batch_size=32)

    model.eval()
    preds_list = []
    with torch.no_grad():
        for batch in tqdm(final_loader, desc="Predicting FINAL dataset"):
            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}
            outputs = model(**inputs)
            preds = torch.argmax(outputs.logits, dim=1)
            preds_list.extend(preds.cpu().numpy())

    # ê²°ê³¼ ì €ì¥
    final_df['predicted_label'] = preds_list
    final_df.to_csv(output_pred_path, index=False)
    print(f"ğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_pred_path}")
    print("========================================\n")

    return accuracy

# 4ï¸âƒ£ ì´ì§„ ë¶„ë¥˜ (binary)
binary_acc = train_and_evaluate(
    "BERT_labeled_binary.csv",
    num_labels=2,
    output_pred_path="predicted_binary_0.csv"
)

# 5ï¸âƒ£ ì‚¼ë¶„ë¥˜ (three-class)
three_acc = train_and_evaluate(
    "BERT_labeled_three.csv",
    num_labels=3,
    output_pred_path="predicted_three_0.csv"
)

print("ğŸ¯ ìµœì¢… ê²°ê³¼")
print(f"Binary Validation Accuracy : {binary_acc:.4f}")
print(f"Three-class Validation Accuracy : {three_acc:.4f}")
