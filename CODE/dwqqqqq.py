# optimized_electra_pipeline.py
import os
import random
from collections import Counter

import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
from sklearn.model_selection import train_test_split
from transformers import (
    ElectraTokenizer,
    ElectraForSequenceClassification,
    get_linear_schedule_with_warmup,
)
from torch.optim import AdamW
from tqdm import tqdm

# ==========================
# ì„¤ì • (í•„ìš”ì‹œ ìˆ˜ì •)
# ==========================
MODEL_NAME = "monologg/koelectra-base-v3-discriminator"
THREE_FILE = "BERT_labeled_three.csv"
FINAL_FILE = "FINAL_DATA_FILTERED_#TRUE.csv"
OUTPUT_FILE = "FINAL_no_neutral_binary_prediction_v3_optimized.csv"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
MAX_LEN = 256               # 128 -> 256ìœ¼ë¡œ ëŠ˜ë¦¼
TRAIN_BATCH_SIZE = 16
VALID_BATCH_SIZE = 32
LR = 1e-5
EPOCHS = 8                  # epochs ì ì ˆíˆ ì„¤ì • (ë„ˆ ìƒí™©ì— ë§ê²Œ)
ACCUMULATION_STEPS = 2      # effective batch size ì¦ê°€
WARMUP_RATIO = 0.06         # scheduler warmup
WEIGHT_DECAY = 0.01
SEED = 42
SAVE_DIR = "./saved_models"
os.makedirs(SAVE_DIR, exist_ok=True)

# ê³ ì •ì‹œë“œ
def set_seed(seed=SEED):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

set_seed()

# ==========================
# ê°„ë‹¨í•œ EDA ì¦ê°• (ë°ì´í„° ì ì„ ë•Œ ì‚¬ìš©)
# ë§¤ìš° ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ê³„ (ì˜ë¯¸ í›¼ì† ìµœì†Œí™”)
# ==========================
def random_deletion(words, p=0.1):
    if len(words) == 1:
        return words
    new_words = [w for w in words if random.random() > p]
    if len(new_words) == 0:
        return [random.choice(words)]
    return new_words

def random_swap(words, n_swaps=1):
    words = words.copy()
    for _ in range(n_swaps):
        i, j = random.sample(range(len(words)), 2)
        words[i], words[j] = words[j], words[i]
    return words

def eda_augment(text, prob_del=0.08, prob_swap=0.06):
    # ë§¤ìš° ë³´ìˆ˜ì : í† í°ì„ ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ” (í•œêµ­ì–´ì—ì„œ ì™„ë²½í•˜ì§„ ì•ŠìŒ)
    words = text.split()
    if len(words) <= 2:
        return text
    if random.random() < 0.5:
        words = random_deletion(words, p=prob_del)
    if random.random() < 0.5:
        words = random_swap(words, n_swaps=1)
    return " ".join(words)

# ==========================
# Dataset
# ==========================
class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=MAX_LEN, augment=False):
        self.texts = list(texts)
        self.labels = None if labels is None else list(labels)
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.augment = augment

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        if self.augment and self.labels is not None:
            # ì¦ê°•ì€ í•™ìŠµ ë°ì´í„°ì—ì„œë§Œ ì ìš©
            text = eda_augment(text)

        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            max_length=self.max_len,
            padding="max_length",
            truncation=True,
        )
        item = {k: v.squeeze(0) for k, v in inputs.items()}
        if self.labels is not None:
            item["labels"] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item

# ==========================
# LLRD (Layer-wise LR decay) í•¨ìˆ˜
# ==========================
def get_optimizer_grouped_parameters(model, base_lr, layer_decay=0.95):
    # model.electra.encoder.layerëŠ” bottom -> top ìˆœì„œ
    # embeddings, encoder.layer[0] ... encoder.layer[-1], classifier
    lr = base_lr
    no_decay = ["bias", "LayerNorm.weight"]

    # collect layers
    layers = []
    layers.append((model.electra.embeddings, "embeddings"))
    # encoder layers
    for i, layer in enumerate(model.electra.encoder.layer):
        layers.append((layer, f"encoder.layer.{i}"))
    # pooler/other is absent for ELECTRA; classifier last
    grouped_parameters = []

    # assign lr with decay: lower layers smaller lr
    n_layers = len(layers)
    for i, (layer_module, name) in enumerate(layers):
        # decay factor increases for lower layers (i small => lower lr)
        scale = layer_decay ** (n_layers - i - 1)
        layer_lr = lr * scale
        grouped_parameters.append(
            {
                "params": [p for n, p in layer_module.named_parameters() if not any(nd in n for nd in no_decay)],
                "weight_decay": WEIGHT_DECAY,
                "lr": layer_lr,
            }
        )
        grouped_parameters.append(
            {
                "params": [p for n, p in layer_module.named_parameters() if any(nd in n for nd in no_decay)],
                "weight_decay": 0.0,
                "lr": layer_lr,
            }
        )

    # classifier params - top lr
    if hasattr(model, "classifier"):
        grouped_parameters.append(
            {
                "params": [p for n, p in model.classifier.named_parameters() if not any(nd in n for nd in no_decay)],
                "weight_decay": WEIGHT_DECAY,
                "lr": lr,
            }
        )
        grouped_parameters.append(
            {
                "params": [p for n, p in model.classifier.named_parameters() if any(nd in n for nd in no_decay)],
                "weight_decay": 0.0,
                "lr": lr,
            }
        )

    return grouped_parameters

# ==========================
# í•™ìŠµ/í‰ê°€ ë£¨í‹´
# - model(..., labels=...)ë¡œ ë‚´ë¶€ loss ì‚¬ìš© (HuggingFace ê¶Œì¥)
# - WeightedRandomSamplerë¡œ í´ë˜ìŠ¤ ë¶ˆê· í˜• ë³´ì •
# - gradient accumulation ì ìš©
# - scheduler ì ìš©
# ==========================
def train_and_evaluate(train_texts, train_labels, val_texts, val_labels, num_labels, config):
    print(f"Training config: LR={config['lr']}, epochs={config['epochs']}, max_len={config['max_len']}")
    tokenizer = ElectraTokenizer.from_pretrained(MODEL_NAME)
    model = ElectraForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels, use_safetensors=True)
    model.to(DEVICE)

    # Dataset & Sampler (WeightedRandomSampler for imbalance)
    train_dataset = TextDataset(train_texts, train_labels, tokenizer, max_len=config["max_len"], augment=True)
    val_dataset = TextDataset(val_texts, val_labels, tokenizer, max_len=config["max_len"], augment=False)

    # Weighted sampler
    class_counts = Counter(train_labels)
    sample_weights = [1.0 / class_counts[l] for l in train_labels]
    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)

    train_loader = DataLoader(train_dataset, batch_size=config["train_bs"], sampler=sampler, drop_last=False)
    val_loader = DataLoader(val_dataset, batch_size=config["valid_bs"], shuffle=False)

    # Optimizer with LLRD parameter groups
    optimizer_grouped_parameters = get_optimizer_grouped_parameters(model, base_lr=config["lr"], layer_decay=0.95)
    optimizer = AdamW(optimizer_grouped_parameters, lr=config["lr"], weight_decay=WEIGHT_DECAY)

    # Scheduler
    total_steps = (len(train_loader) // config["accum_steps"] + (1 if len(train_loader) % config["accum_steps"] else 0)) * config["epochs"]
    warmup_steps = max(1, int(total_steps * config["warmup_ratio"]))
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)

    best_val_acc = 0.0
    best_model_path = None

    for epoch in range(config["epochs"]):
        model.train()
        running_loss = 0.0
        optimizer.zero_grad()

        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f"Epoch {epoch+1}/{config['epochs']}")
        for step, batch in pbar:
            # move to device
            inputs = {k: v.to(DEVICE) for k, v in batch.items()}
            outputs = model(**inputs)  # labels í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ outputs.loss ì‚¬ìš©
            loss = outputs.loss / config["accum_steps"]
            loss.backward()
            running_loss += loss.item() * config["accum_steps"]

            if (step + 1) % config["accum_steps"] == 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()

                pbar.set_postfix({"loss": f"{running_loss / ((step+1)):.4f}"})

        # --- validation ---
        model.eval()
        correct, total = 0, 0
        with torch.no_grad():
            for batch in val_loader:
                inputs = {k: v.to(DEVICE) for k, v in batch.items()}
                outputs = model(**inputs)
                preds = torch.argmax(outputs.logits, dim=1)
                correct += (preds.cpu() == inputs["labels"].cpu()).sum().item()
                total += len(preds)
        val_acc = correct / total if total > 0 else 0.0
        print(f"Epoch {epoch+1} validation accuracy: {val_acc:.4f}")

        # save best
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_model_path = os.path.join(SAVE_DIR, f"best_model_epoch{epoch+1}_acc{val_acc:.4f}.safetensors")
            # save using save_pretrained (will create pytorch_model.bin by default) - prefer save_pretrained
            model.save_pretrained(os.path.dirname(best_model_path), safe_serialization=True)
            print(f"Saved best model -> {os.path.dirname(best_model_path)}")

    return model, tokenizer, best_val_acc, best_model_path

# ==========================
# ì˜ˆì¸¡ í•¨ìˆ˜ (ë°ì´í„°í”„ë ˆì„ ê¸°ë°˜)
# ==========================
def predict_data(model, tokenizer, df_target, output_path=None, max_len=MAX_LEN, batch_size=VALID_BATCH_SIZE):
    texts = df_target["text"].astype(str).tolist()
    dataset = TextDataset(texts, None, tokenizer, max_len=max_len, augment=False)
    loader = DataLoader(dataset, batch_size=batch_size)

    model.eval()
    preds = []
    with torch.no_grad():
        for batch in tqdm(loader, desc="Predicting"):
            inputs = {k: v.to(DEVICE) for k, v in batch.items()}
            outputs = model(**inputs)
            preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())

    df_target = df_target.copy()
    df_target["predicted_label"] = preds

    if output_path:
        df_target.to_csv(output_path, index=False, encoding="utf-8")
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")

    return df_target

# ==========================
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ==========================
def main():
    print("Using device:", DEVICE)
    # Step 1. ë¼ë²¨ë§ ë°ì´í„° ë¡œë“œ
    df_three = pd.read_csv(THREE_FILE)
    print(f"ë¼ë²¨ë§ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df_three)}ê°œ")

    # Step 2. ì¤‘ë¦½ vs ë¹„ì¤‘ë¦½ (1ë‹¨ê³„)
    df_three["is_neutral"] = df_three["sentiment_three"].apply(lambda x: 1 if x == "ì¤‘ë¦½" else 0)
    df_temp = df_three[["text", "is_neutral"]].reset_index(drop=True)
    texts = df_temp["text"].tolist()
    labels = df_temp["is_neutral"].tolist()

    train_t, val_t, train_l, val_l = train_test_split(texts, labels, test_size=0.2, random_state=SEED, stratify=labels)

    print("1ë‹¨ê³„ í•™ìŠµ ë¼ë²¨ ë¶„í¬:", Counter(train_l))

    config_stage1 = {
        "lr": LR,
        "epochs": max(3, EPOCHS//2),  # ì¤‘ë¦½í•„í„°ëŠ” epoch ì‘ê²Œë„ ì‹œë„ ê°€ëŠ¥
        "train_bs": TRAIN_BATCH_SIZE,
        "valid_bs": VALID_BATCH_SIZE,
        "accum_steps": ACCUMULATION_STEPS,
        "warmup_ratio": WARMUP_RATIO,
        "max_len": MAX_LEN,
    }

    neutral_model, neutral_tokenizer, neutral_acc, _ = train_and_evaluate(train_t, train_l, val_t, val_l, num_labels=2, config=config_stage1)
    print(f"âœ… 1ë‹¨ê³„ (ì¤‘ë¦½ í•„í„°ë§) ìµœê³  Validation Accuracy: {neutral_acc:.4f}")

    # Step 3. ì›ë³¸ ë°ì´í„° ë¡œë“œ & ì¤‘ë¦½ ì œê±°
    df_final = pd.read_csv(FINAL_FILE)
    df_pred_neutral = predict_data(neutral_model, neutral_tokenizer, df_final, output_path=None)
    df_filtered = df_pred_neutral[df_pred_neutral["predicted_label"] == 0].copy()
    print(f"ì´ {len(df_final)} â†’ ì¤‘ë¦½ ì œê±° í›„ {len(df_filtered)}ê°œ ë‚¨ìŒ")

    # Step 4. ê¸/ë¶€ì • í•™ìŠµ ë°ì´í„° ì¤€ë¹„ (2ë‹¨ê³„)
    df_pure = df_three[df_three["sentiment_three"] != "ì¤‘ë¦½"].copy().reset_index(drop=True)
    df_pure["label"] = df_pure["sentiment_three"].map({"ë¶€ì •": 0, "ê¸ì •": 1})
    texts_p = df_pure["text"].tolist()
    labels_p = df_pure["label"].tolist()

    tp_t, vp_t, tp_l, vp_l = train_test_split(texts_p, labels_p, test_size=0.2, random_state=SEED, stratify=labels_p)
    print("2ë‹¨ê³„ í•™ìŠµ ë¼ë²¨ ë¶„í¬:", Counter(tp_l))

    config_stage2 = {
        "lr": LR,
        "epochs": EPOCHS,
        "train_bs": TRAIN_BATCH_SIZE,
        "valid_bs": VALID_BATCH_SIZE,
        "accum_steps": ACCUMULATION_STEPS,
        "warmup_ratio": WARMUP_RATIO,
        "max_len": MAX_LEN,
    }

    binary_model, binary_tokenizer, binary_acc, best_path = train_and_evaluate(tp_t, tp_l, vp_t, vp_l, num_labels=2, config=config_stage2)
    print(f"âœ… 2ë‹¨ê³„ (ê¸/ë¶€ì •) ìµœê³  Validation Accuracy: {binary_acc:.4f}")

    # Step 5. ìµœì¢… ì˜ˆì¸¡ & ì €ì¥
    df_final_pred = predict_data(binary_model, binary_tokenizer, df_filtered, output_path=OUTPUT_FILE)
    print("\n" + "=" * 60)
    print("ğŸ¯ ìµœì¢… ë³´ê³  ìš”ì•½")
    print(f"1ë‹¨ê³„ (ì¤‘ë¦½ í•„í„°ë§) Best Validation Accuracy: {neutral_acc:.4f}")
    print(f"2ë‹¨ê³„ (ê¸/ë¶€ì •) Best Validation Accuracy: {binary_acc:.4f}")
    print(f"ìµœì¢… ë¶„ì„ ë°ì´í„° ìˆ˜ (ì •ì œ í›„): {len(df_final_pred)}")
    print("=" * 60)

if __name__ == "__main__":
    main()
