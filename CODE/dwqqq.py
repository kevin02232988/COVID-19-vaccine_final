import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from transformers import ElectraTokenizer, ElectraForSequenceClassification
from torch.optim import AdamW
from torch.nn.functional import softmax
from tqdm import tqdm
from collections import Counter
import os

# ======================================================
# 1ï¸âƒ£ GPU ì„¤ì •
# ======================================================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)


# ======================================================
# 2ï¸âƒ£ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
# ======================================================
class TextDataset(Dataset):
    """PyTorch í•™ìŠµ/ê²€ì¦ìš© ë°ì´í„°ì…‹"""

    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        inputs = self.tokenizer(
            text, return_tensors='pt', max_length=self.max_len, padding='max_length', truncation=True
        )
        item = {key: val.squeeze(0) for key, val in inputs.items()}
        if self.labels is not None:
            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item


# ======================================================
# 3ï¸âƒ£ í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜
# ======================================================
def train_and_evaluate(train_texts, train_labels, val_texts, val_labels, num_labels, custom_lr, epochs, weights_tensor):
    MODEL_NAME = "monologg/koelectra-base-v3-discriminator"
    tokenizer = ElectraTokenizer.from_pretrained(MODEL_NAME)

    train_dataset = TextDataset(train_texts, train_labels, tokenizer)
    val_dataset = TextDataset(val_texts, val_labels, tokenizer)

    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32)

    model = ElectraForSequenceClassification.from_pretrained(
        MODEL_NAME, num_labels=num_labels, use_safetensors=True
    ).to(device)

    loss_fn = torch.nn.CrossEntropyLoss(weight=weights_tensor)
    optimizer = AdamW(model.parameters(), lr=custom_lr)

    # === í•™ìŠµ ë£¨í”„ ===
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch in tqdm(train_loader, desc=f"Epoch {epoch + 1}"):
            optimizer.zero_grad()
            inputs = {k: v.to(device) for k, v in batch.items()}
            outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])
            loss = loss_fn(outputs.logits, inputs['labels'])
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

    # === ê²€ì¦ ===
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for batch in val_loader:
            inputs = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**inputs)
            preds = torch.argmax(outputs.logits, dim=1)
            correct += (preds.cpu() == inputs['labels'].cpu()).sum().item()
            total += len(preds)
    accuracy = correct / total

    return model, accuracy, tokenizer


# ======================================================
# 4ï¸âƒ£ ì˜ˆì¸¡ í•¨ìˆ˜ (í™•ì‹ ë„ ê¸°ë°˜ í•„í„°ë§ ì¶”ê°€)
# ======================================================
def predict_data(model, tokenizer, df_target, output_pred_path=None):
    final_texts = df_target['text'].astype(str).tolist()
    pred_dataset = TextDataset(final_texts, None, tokenizer)
    pred_loader = DataLoader(pred_dataset, batch_size=32)

    model.eval()
    preds_list, probs_list = [], []
    with torch.no_grad():
        for batch in tqdm(pred_loader, desc="Predicting dataset"):
            inputs = {k: v.to(device) for k, v in batch.items()}
            outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])
            probs = softmax(outputs.logits, dim=1)
            preds = torch.argmax(probs, dim=1)
            preds_list.extend(preds.cpu().numpy())
            probs_list.extend(probs[:, 1].cpu().numpy())  # ì¤‘ë¦½ í™•ë¥ 

    df_target['predicted_label'] = preds_list
    df_target['neutral_prob'] = probs_list

    if output_pred_path:
        df_target.to_csv(output_pred_path, index=False, encoding='utf-8')
        print(f"ğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_pred_path}")

    return df_target


# ======================================================
# 5ï¸âƒ£ ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
# ======================================================
THREE_CLASS_FILE = "BERT_labeled_three.csv"
FINAL_DATA_FILE = "FINAL_DATA_FILTERED_#TRUE.csv"

# === 1ë‹¨ê³„: ì¤‘ë¦½ vs ë¹„ì¤‘ë¦½ í•™ìŠµ ===
try:
    df_three = pd.read_csv(THREE_CLASS_FILE, encoding='utf-8')
except Exception:
    df_three = pd.read_csv(THREE_CLASS_FILE, encoding='cp949')

df_three['is_neutral'] = df_three['sentiment_three'].apply(lambda x: 1 if x == 'ì¤‘ë¦½' else 0)

neutral_texts = df_three['text'].tolist()
neutral_labels = df_three['is_neutral'].tolist()

tn_texts, vn_texts, tn_labels, vn_labels = train_test_split(
    neutral_texts, neutral_labels, test_size=0.2, random_state=42, stratify=neutral_labels
)

neutral_counts = Counter(tn_labels)
tn_total = len(tn_labels)
neutral_weights = {i: tn_total / (len(neutral_counts) * count) for i, count in neutral_counts.items()}
neutral_weights_tensor = torch.tensor([neutral_weights[i] for i in sorted(neutral_weights.keys())],
                                      dtype=torch.float).to(device)

print("\n" + "=" * 50)
print("1ë‹¨ê³„: ì¤‘ë¦½ ë¶„ë¥˜ê¸° í•™ìŠµ ì‹œì‘ (ë¹„ì¤‘ë¦½=0, ì¤‘ë¦½=1)")
print(f"Original Training Distribution: {neutral_counts}")
print(f"Calculated Class Weights: {neutral_weights_tensor}")

neutral_model, neutral_acc, neutral_tokenizer = train_and_evaluate(
    tn_texts, tn_labels, vn_texts, vn_labels,
    num_labels=2, custom_lr=3e-5, epochs=6, weights_tensor=neutral_weights_tensor
)
print(f"âœ… 1ë‹¨ê³„ ê²€ì¦ ì •í™•ë„ (ì¤‘ë¦½ ì—¬ë¶€): {neutral_acc:.4f}")


# === 2ë‹¨ê³„: ì›ë³¸ ë°ì´í„° ì •ì œ ===
print("\n" + "=" * 50)
print("2ë‹¨ê³„: ì¤‘ë¦½ ë¶„ë¥˜ê¸°ë¡œ ì›ë³¸ ë°ì´í„° ì •ì œ ì‹œì‘")

try:
    df_final = pd.read_csv(FINAL_DATA_FILE, encoding='utf-8')
except Exception:
    df_final = pd.read_csv(FINAL_DATA_FILE, encoding='cp949')

df_predicted_neutral = predict_data(neutral_model, neutral_tokenizer, df_final)

# âœ… í™•ì‹  ë†’ì€ ì¤‘ë¦½(0.75 ì´ìƒ)ë§Œ ì œê±°
df_purified = df_predicted_neutral[df_predicted_neutral['neutral_prob'] < 0.75].copy()
df_purified.drop(columns=['predicted_label', 'neutral_prob'], inplace=True)

print(f"ì´ ì›ë³¸ ë°ì´í„°: {len(df_final)}ê°œ, ì¤‘ë¦½ í™•ì‹ >0.75 ë°ì´í„° ì‚­ì œ í›„: {len(df_purified)}ê°œ")


# === 3ë‹¨ê³„: ê¸ì •/ë¶€ì • ë¶„ë¥˜ í•™ìŠµ ===
df_labeled = df_three[df_three['sentiment_three'] != 'ì¤‘ë¦½'].copy()
df_labeled.rename(columns={'sentiment_three': 'sentiment_purified'}, inplace=True)

purified_texts = df_labeled['text'].tolist()
purified_labels_text = df_labeled['sentiment_purified'].tolist()
purified_labels = [0 if l == 'ë¶€ì •' else 1 for l in purified_labels_text]

tp_texts, vp_texts, tp_labels, vp_labels = train_test_split(
    purified_texts, purified_labels, test_size=0.2, random_state=42, stratify=purified_labels
)

purified_counts = Counter(tp_labels)
tp_total = len(tp_labels)
purified_weights = {i: tp_total / (len(purified_counts) * count) for i, count in purified_counts.items()}
purified_weights_tensor = torch.tensor([purified_weights[i] for i in sorted(purified_counts.keys())],
                                       dtype=torch.float).to(device)

print("\n" + "=" * 50)
print("3ë‹¨ê³„: ìˆœìˆ˜ Binary ë¶„ë¥˜ê¸° í•™ìŠµ ì‹œì‘")
print(f"Original Training Distribution: {purified_counts}")
print(f"Calculated Class Weights: {purified_weights_tensor}")

final_binary_model, final_binary_acc, final_binary_tokenizer = train_and_evaluate(
    tp_texts, tp_labels, vp_texts, vp_labels,
    num_labels=2, custom_lr=2e-5, epochs=5, weights_tensor=purified_weights_tensor
)
print(f"âœ… 3ë‹¨ê³„ ìµœì¢… ê²€ì¦ ì •í™•ë„: {final_binary_acc:.4f}")


# === 4ë‹¨ê³„: ìµœì¢… ì˜ˆì¸¡ ë° ì €ì¥ ===
print("\n" + "=" * 50)
print("4ë‹¨ê³„: ìµœì¢… ì •ì œëœ ë°ì´í„°ì…‹ ì˜ˆì¸¡ ë° ì €ì¥")

df_final_predicted = predict_data(
    final_binary_model,
    final_binary_tokenizer,
    df_purified,
    "predicted_final_purified_binary_v2.csv"
)

print(f"âœ… ìµœì¢… ê²°ê³¼ íŒŒì¼ ì €ì¥ ì™„ë£Œ: predicted_final_purified_binary_v2.csv")
print(f"ì´ ìµœì¢… ë¶„ì„ ëŒ“ê¸€ ìˆ˜: {len(df_final_predicted)}")

print("\nğŸ¯ ìµœì¢… ë³´ê³ ìš© ê²°ê³¼")
print(f"1ë‹¨ê³„ (ì¤‘ë¦½ ë¶„ë¥˜) ì •í™•ë„: {neutral_acc:.4f}")
print(f"3ë‹¨ê³„ (Binary) ì •í™•ë„: {final_binary_acc:.4f}")
