import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from transformers import ElectraTokenizer, ElectraForSequenceClassification
from torch.optim import AdamW
from tqdm import tqdm
from collections import Counter

# 1ï¸âƒ£ GPU ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)


# 2ï¸âƒ£ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜ (NameError ë°©ì§€ë¥¼ ìœ„í•´ í•¨ìˆ˜ ë‚´ë¶€ë¡œ ì´ë™)
def train_and_evaluate(csv_path, num_labels, output_pred_path):
    print(f"\n===== {csv_path} ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ëª¨ë¸: KoElectra Base, í´ë˜ìŠ¤ ìˆ˜: {num_labels}) =====")

    # 2ï¸âƒ£ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
    class TextDataset(Dataset):
        """PyTorch í•™ìŠµ/ê²€ì¦ìš© ë°ì´í„°ì…‹"""

        def __init__(self, texts, labels, tokenizer, max_len=128):
            self.texts = texts
            self.labels = labels
            self.tokenizer = tokenizer
            self.max_len = max_len

        def __len__(self):
            return len(self.texts)

        def __getitem__(self, idx):
            text = str(self.texts[idx])
            inputs = self.tokenizer(
                text, return_tensors='pt', max_length=self.max_len, padding='max_length', truncation=True
            )
            item = {key: val.squeeze(0) for key, val in inputs.items()}
            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
            return item

    # ë°ì´í„° ë¡œë“œ (ì´ì „ê³¼ ë™ì¼)
    df = pd.read_csv(csv_path)

    # CSV ì´ë¦„ì— ë”°ë¼ ë¼ë²¨ ì»¬ëŸ¼ê³¼ Learning Rate ì„¤ì •
    if "binary" in csv_path.lower():
        label_col = "sentiment_binary";
        label_map = {"ë¶€ì •": 0, "ê¸ì •": 1}
        custom_lr = 2e-5  # ì´ì „ ìµœê³  ì„±ëŠ¥ LR

    elif "three" in csv_path.lower():
        label_col = "sentiment_three";
        label_map = {"ë¶€ì •": 0, "ì¤‘ë¦½": 1, "ê¸ì •": 2}
        custom_lr = 1e-5  # ì•ˆì •ì ì¸ ë‚®ì€ LR

    else:
        raise ValueError("CSV íŒŒì¼ ì´ë¦„ì— 'binary' ë˜ëŠ” 'three'ê°€ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")

    if label_col not in df.columns:
        raise ValueError(f"'{label_col}' ì»¬ëŸ¼ì´ CSVì— ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ì»¬ëŸ¼: {df.columns.tolist()}")

    texts = df['text'].astype(str).tolist()
    labels = [label_map[l] for l in df[label_col].tolist()]

    # 3.1. í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„ë¦¬
    train_texts, val_texts, train_labels, val_labels = train_test_split(
        texts, labels, test_size=0.2, random_state=42, stratify=labels
    )

    print(f"Original Training Distribution: {Counter(train_labels)}")

    # 3.2. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ë° ì ìš© (ê°€ì¥ ì•ˆì •ì ì¸ ìë™ ì—­ë¹ˆë„ ê°€ì¤‘ì¹˜ë¡œ ë³µê·€)
    class_counts = Counter(train_labels)
    total_samples = len(train_labels)

    # ìë™ ì—­ë¹ˆë„ ê°€ì¤‘ì¹˜ ê³„ì‚°
    class_weights = {i: total_samples / (num_labels * count) for i, count in class_counts.items()}
    weights_tensor = torch.tensor([class_weights[i] for i in sorted(class_counts.keys())], dtype=torch.float).to(device)
    print(f"Calculated Auto Class Weights (Order 0, 1, ...): {weights_tensor}")

    # 3.3. í† í¬ë‚˜ì´ì € ë° ë°ì´í„°ì…‹
    MODEL_NAME = "monologg/koelectra-base-v3-discriminator"
    tokenizer = ElectraTokenizer.from_pretrained(MODEL_NAME)

    train_dataset = TextDataset(train_texts, train_labels, tokenizer)
    val_dataset = TextDataset(val_texts, val_labels, tokenizer)

    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32)

    # 3.4. ëª¨ë¸ ë° ì†ì‹¤ í•¨ìˆ˜
    model = ElectraForSequenceClassification.from_pretrained(
        MODEL_NAME,
        num_labels=num_labels,
        use_safetensors=True
    ).to(device)

    # === í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ë¥¼ Loss Functionì— ì ìš© ===
    loss_fn = torch.nn.CrossEntropyLoss(weight=weights_tensor)

    optimizer = AdamW(model.parameters(), lr=custom_lr)
    epochs = 5  # === ì—í¬í¬ë¥¼ 5ë¡œ ë³µê·€ ===

    # 3.5. í•™ìŠµ ë£¨í”„ (ì´ì „ê³¼ ë™ì¼)
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch in tqdm(train_loader, desc=f"Epoch {epoch + 1}"):
            optimizer.zero_grad()
            inputs = {k: v.to(device) for k, v in batch.items()}

            outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])
            loss = loss_fn(outputs.logits, inputs['labels'])

            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch + 1} | Train Loss: {total_loss / len(train_loader):.4f}")

    # 3.6. ê²€ì¦ (ì´ì „ê³¼ ë™ì¼)
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for batch in val_loader:
            inputs = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**inputs)
            preds = torch.argmax(outputs.logits, dim=1)
            correct += (preds.cpu() == inputs['labels'].cpu()).sum().item()
            total += len(preds)
    accuracy = correct / total
    print(f"âœ… Validation Accuracy: {accuracy:.4f}")

    # 3.7. ì›ë³¸ ë°ì´í„° ì˜ˆì¸¡ (ì´ì „ê³¼ ë™ì¼)
    final_df = pd.read_csv("FINAL_DATA_FILTERED_#TRUE.csv")
    final_texts = final_df['text'].astype(str).tolist()
    final_dataset = TextDataset(final_texts, [0] * len(final_texts), tokenizer)
    final_loader = DataLoader(final_dataset, batch_size=32)

    model.eval()
    preds_list = []
    with torch.no_grad():
        for batch in tqdm(final_loader, desc="Predicting FINAL dataset"):
            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}
            outputs = model(**inputs)
            preds = torch.argmax(outputs.logits, dim=1)
            preds_list.extend(preds.cpu().numpy())

    # ê²°ê³¼ ì €ì¥ (íŒŒì¼ëª… ë³€ê²½)
    final_df['predicted_label'] = preds_list
    final_df.to_csv(output_pred_path, index=False)
    print(f"ğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_pred_path}")
    print("========================================\n")

    return accuracy


# 4ï¸âƒ£ ì´ì§„ ë¶„ë¥˜ (binary)
binary_acc = train_and_evaluate(
    "BERT_labeled_binary.csv",
    num_labels=2,
    output_pred_path="predicted_binary_koelectra_stable_1.csv"
)

# 5ï¸âƒ£ ì‚¼ë¶„ë¥˜ (three-class)
three_acc = train_and_evaluate(
    "BERT_labeled_three.csv",
    num_labels=3,
    output_pred_path="predicted_three_koelectra_stable_1.csv"
)

print("ğŸ¯ ìµœì¢… ê²°ê³¼")
print(f"Binary Validation Accuracy : {binary_acc:.4f}")
print(f"Three-class Validation Accuracy : {three_acc:.4f}")