import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from transformers import ElectraTokenizer, ElectraForSequenceClassification
from torch.optim import AdamW
from tqdm import tqdm
from collections import Counter
import os

# =========================================
# 1ï¸âƒ£ GPU ì„¤ì •
# =========================================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)


# =========================================
# 2ï¸âƒ£ Dataset í´ë˜ìŠ¤ ì •ì˜
# =========================================
class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        # textsë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ í™•ì •í•˜ì—¬ Pandas ì¸ë±ìŠ¤ ë¬¸ì œë¥¼ ë°©ì§€ (Safe conversion to list)
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        # textsì™€ labelsê°€ ëª¨ë‘ ìˆœìˆ˜ ë¦¬ìŠ¤íŠ¸ì„ì„ ê°€ì •
        text = str(self.texts[idx])
        inputs = self.tokenizer(
            text, return_tensors="pt", max_length=self.max_len, padding="max_length", truncation=True
        )
        item = {key: val.squeeze(0) for key, val in inputs.items()}
        if self.labels is not None:
            item["labels"] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item


# =========================================
# 3ï¸âƒ£ í•™ìŠµ + í‰ê°€ í•¨ìˆ˜
# =========================================
def train_and_evaluate(train_texts, train_labels, val_texts, val_labels, num_labels, lr, epochs, weights_tensor):
    print(f"| LR: {lr}, Epochs: {epochs}")
    MODEL_NAME = "monologg/koelectra-base-v3-discriminator"
    tokenizer = ElectraTokenizer.from_pretrained(MODEL_NAME)

    train_dataset = TextDataset(train_texts, train_labels, tokenizer)
    val_dataset = TextDataset(val_texts, val_labels, tokenizer)

    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32)

    model = ElectraForSequenceClassification.from_pretrained(
        MODEL_NAME, num_labels=num_labels, use_safetensors=True
    ).to(device)

    loss_fn = torch.nn.CrossEntropyLoss(weight=weights_tensor)
    optimizer = AdamW(model.parameters(), lr=lr)

    # === í•™ìŠµ ===
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}"):
            optimizer.zero_grad()
            inputs = {k: v.to(device) for k, v in batch.items()}
            outputs = model(input_ids=inputs["input_ids"], attention_mask=inputs["attention_mask"])
            loss = loss_fn(outputs.logits, inputs["labels"])
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

    # === ê²€ì¦ ===
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for batch in val_loader:
            inputs = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**inputs)
            preds = torch.argmax(outputs.logits, dim=1)
            correct += (preds.cpu() == inputs["labels"].cpu()).sum().item()
            total += len(preds)
    acc = correct / total

    return model, acc, tokenizer


# =========================================
# 4ï¸âƒ£ ì˜ˆì¸¡ í•¨ìˆ˜
# =========================================
def predict_data(model, tokenizer, df_target, output_path=None):
    # ì˜ˆì¸¡ ì‹œì—ëŠ” ì¸ë±ìŠ¤ ë¬¸ì œê°€ ì—†ë„ë¡ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    texts = df_target["text"].astype(str).tolist()
    dataset = TextDataset(texts, None, tokenizer)
    loader = DataLoader(dataset, batch_size=32)

    model.eval()
    preds = []
    with torch.no_grad():
        for batch in tqdm(loader, desc="Predicting"):
            inputs = {k: v.to(device) for k, v in batch.items()}
            outputs = model(input_ids=inputs["input_ids"], attention_mask=inputs["attention_mask"])
            preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())

    df_target["predicted_label"] = preds

    if output_path:
        df_target.to_csv(output_path, index=False, encoding="utf-8")
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")

    return df_target


# =========================================
# 5ï¸âƒ£ Main Pipeline
# =========================================

THREE_FILE = "BERT_labeled_three.csv"
FINAL_FILE = "FINAL_DATA_FILTERED_#TRUE.csv"

# === Step 1. ë°ì´í„° ë¡œë“œ ===
df_three = pd.read_csv(THREE_FILE)
print(f"ë¼ë²¨ë§ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df_three)}ê°œ")

# === Step 2. ì¤‘ë¦½ vs ë¹„ì¤‘ë¦½ (1ë‹¨ê³„) ===
df_three["is_neutral"] = df_three["sentiment_three"].apply(lambda x: 1 if x == "ì¤‘ë¦½" else 0)

# ì¸ë±ìŠ¤ ì´ˆê¸°í™” ë° ë¦¬ìŠ¤íŠ¸ ë³€í™˜ (KeyError ë°©ì§€)
df_temp = df_three[['text', 'is_neutral']].reset_index(drop=True)
texts = df_temp["text"].tolist()
labels = df_temp["is_neutral"].tolist()

train_t, val_t, train_l, val_l = train_test_split(texts, labels, test_size=0.2, random_state=42, stratify=labels)

counts = Counter(train_l)
weights = {i: len(train_l) / (2 * c) for i, c in counts.items()}
weights_tensor = torch.tensor([weights[i] for i in sorted(weights.keys())], dtype=torch.float).to(device)

print(f"1ë‹¨ê³„ í•™ìŠµ ë¼ë²¨ ë¶„í¬: {counts}")

# 1ë‹¨ê³„ íŠœë‹: LRì„ ì•ˆì •í™”í•˜ê³  Epochë¥¼ 10íšŒë¡œ ì¦ê°€
neutral_model, neutral_acc, neutral_tokenizer = train_and_evaluate(
    train_t, train_l, val_t, val_l, num_labels=2, lr=1e-5, epochs=10, weights_tensor=weights_tensor
)
print(f"âœ… 1ë‹¨ê³„ (ì¤‘ë¦½ í•„í„°ë§) Validation Accuracy: {neutral_acc:.4f}")

# === Step 3. ì›ë³¸ ë°ì´í„° ë¡œë“œ & ì¤‘ë¦½ ì œê±° (2ë‹¨ê³„) ===
df_final = pd.read_csv(FINAL_FILE)
df_pred_neutral = predict_data(neutral_model, neutral_tokenizer, df_final, None)

# Soft filtering: ì¤‘ë¦½ í™•ë¥ ì´ ë†’ì€ í•­ëª©ë§Œ ì œê±° (0 = ë¹„ì¤‘ë¦½ë§Œ ë‚¨ê¹€)
df_filtered = df_pred_neutral[df_pred_neutral["predicted_label"] == 0].copy()
print(f"ì´ {len(df_final)} â†’ ì¤‘ë¦½ ì œê±° í›„ {len(df_filtered)}ê°œ ë‚¨ìŒ")

# === Step 4. ê¸/ë¶€ì • í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ===
df_pure = df_three[df_three["sentiment_three"] != "ì¤‘ë¦½"].copy().reset_index(drop=True)
df_pure["label"] = df_pure["sentiment_three"].map({"ë¶€ì •": 0, "ê¸ì •": 1})

# ì¸ë±ìŠ¤ ë¦¬ì…‹ í›„ ë¦¬ìŠ¤íŠ¸ ë³€í™˜
texts_p = df_pure["text"].tolist()
labels_p = df_pure["label"].tolist()

tp_t, vp_t, tp_l, vp_l = train_test_split(texts_p, labels_p, test_size=0.2, random_state=42, stratify=labels_p)

counts2 = Counter(tp_l)
weights2 = {i: len(tp_l) / (2 * c) for i, c in counts2.items()}
weights_tensor2 = torch.tensor([weights2[i] for i in sorted(weights2.keys())], dtype=torch.float).to(device)

print(f"2ë‹¨ê³„ í•™ìŠµ ë¼ë²¨ ë¶„í¬: {counts2}")

# 2ë‹¨ê³„ íŠœë‹: LR=1e-5, Epochs=10 (í•™ìŠµ ê°•í™”)
# 1ë‹¨ê³„ì˜ ì„±ê³µì ì¸ ì„¤ì •ì„ 2ë‹¨ê³„ì— ê·¸ëŒ€ë¡œ ì ìš©í•˜ì—¬ ì•ˆì •í™”ì™€ ì„±ëŠ¥ ê·¹ëŒ€í™”ë¥¼ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.
binary_model, binary_acc, binary_tokenizer = train_and_evaluate(
    tp_t, tp_l, vp_t, vp_l, num_labels=2, lr=1e-5, epochs=10, weights_tensor=weights_tensor2
)
print(f"âœ… 2ë‹¨ê³„ (ê¸/ë¶€ì •) Validation Accuracy: {binary_acc:.4f}")

# === Step 5. ìµœì¢… ì˜ˆì¸¡ & ì €ì¥ ===
OUTPUT_FILE = "FINAL_no_neutral_binary_prediction_v3_fixed.csv"

# ìµœì¢… ì˜ˆì¸¡ì€ ì •ì œëœ ë°ì´í„°í”„ë ˆì„(df_filtered)ì— ëŒ€í•´ ìˆ˜í–‰
df_final_pred = predict_data(binary_model, binary_tokenizer, df_filtered, OUTPUT_FILE)

# ğŸ¯ ë³´ê³  ìš”ì•½
print("\n" + "=" * 60)
print("ğŸ¯ ìµœì¢… ë³´ê³  ìš”ì•½")
print(f"1ë‹¨ê³„ (ì¤‘ë¦½ í•„í„°ë§) Validation Accuracy: {neutral_acc:.4f}")
print(f"2ë‹¨ê³„ (ê¸/ë¶€ì •) Validation Accuracy: {binary_acc:.4f}")
print(f"ìµœì¢… ë¶„ì„ ë°ì´í„° ìˆ˜ (ì •ì œ í›„): {len(df_final_pred)}")
print("=" * 60)