import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from collections import Counter # ë¹ˆë„ ê³„ì‚°ì„ ìœ„í•´ Counter ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
import sys

# ----------------------------------------------------------------------
# 1. ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ (ì´ì „ì— ì‚¬ìš©í•œ ê²ƒê³¼ ë™ì¼)
# ----------------------------------------------------------------------
def preprocess_text(text):
    """í…ìŠ¤íŠ¸ë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜: ì†Œë¬¸ìí™”, ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°, í† í°í™”, ë¶ˆìš©ì–´ ì œê±°, í‘œì œì–´ ì¶”ì¶œ."""
    if not isinstance(text, str):
        return []

    # 1-1. ì†Œë¬¸ìí™” ë° URL/ìˆ«ì/íŠ¹ìˆ˜ë¬¸ì ì œê±°
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', '', text)

    # 1-2. í† í°í™”
    tokens = nltk.word_tokenize(text)

    # 1-3. ë¶ˆìš©ì–´ ì œê±° ë° í‘œì œì–´ ì¶”ì¶œ
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()

    # 1-4. ê¸¸ì´ê°€ 3ì ì´í•˜ì¸ ë‹¨ì–´ì™€ ë¶ˆìš©ì–´ ì œê±°, ê·¸ë¦¬ê³  í‘œì œì–´ ì¶”ì¶œ ì ìš©
    processed_tokens = [
        lemmatizer.lemmatize(token)
        for token in tokens
        if token not in stop_words and len(token) > 3
    ]

    return processed_tokens

# ----------------------------------------------------------------------
# ë©”ì¸ ì‹¤í–‰ ë¸”ë¡ (ë‹¨ì–´ ë¹ˆë„ ë¶„ì„)
# ----------------------------------------------------------------------
if __name__ == '__main__':

    # NLTK ë°ì´í„°ëŠ” ì´ì „ ë‹¨ê³„ì—ì„œ ë‹¤ìš´ë¡œë“œë˜ì—ˆìœ¼ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    # check_and_download_nltk_data()

    file_path = "Real_Final.csv"
    try:
        df = pd.read_csv(file_path)
        print(f"'{file_path}' íŒŒì¼ ë¡œë“œ ì„±ê³µ. ì´ {len(df)}ê°œì˜ ë°ì´í„°.")
    except FileNotFoundError:
        print(f"ì˜¤ë¥˜: íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. '{file_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    # 'text' ì»¬ëŸ¼ì„ ì „ì²˜ë¦¬
    print("í…ìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    df['processed_text'] = df['text'].apply(preprocess_text)

    # 1. ëª¨ë“  ì „ì²˜ë¦¬ëœ ë‹¨ì–´ë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í†µí•©
    all_words = []
    for doc in df['processed_text']:
        if isinstance(doc, list):
            all_words.extend(doc)

    # 2. ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
    print("ë‹¨ì–´ ë¹ˆë„ ê³„ì‚° ì¤‘...")
    word_counts = Counter(all_words)

    # 3. ìƒìœ„ Nê°œ í‚¤ì›Œë“œ ì¶”ì¶œ
    TOP_N = 50
    top_keywords = word_counts.most_common(TOP_N)

    print("-" * 50)
    print(f"ğŸš¨ğŸš¨ğŸš¨ ì´ ë°ì´í„°ì—ì„œ ë¹ˆë„ê°€ ê°€ì¥ ë†’ì€ ìƒìœ„ {TOP_N}ê°œ í‚¤ì›Œë“œ ğŸš¨ğŸš¨ğŸš¨")

    # ê²°ê³¼ë¥¼ ê¹”ë”í•˜ê²Œ ì¶œë ¥
    for i, (word, count) in enumerate(top_keywords):
        # f-string í¬ë§·íŒ…ì„ ì‚¬ìš©í•˜ì—¬ ë²ˆí˜¸, ë‹¨ì–´, íšŸìˆ˜ë¥¼ ì •ë ¬í•˜ì—¬ ì¶œë ¥
        print(f"{i+1:2d}. {word:15s} : {count:,d}íšŒ")

    print("-" * 50)
    print(f"ì „ì²´ ë¬¸ì„œ ìˆ˜: {len(df):,d}ê°œ")
    print(f"ì „ì²´ ê³ ìœ  ë‹¨ì–´ ìˆ˜: {len(word_counts):,d}ê°œ")
    print("âœ¨ ì°¸ê³ : ì´ ëª©ë¡ì€ ë‹¨ìˆœíˆ ë¹ˆë„ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. 'ì˜ë¯¸ ìˆëŠ”' í‚¤ì›Œë“œë¥¼ ì°¾ìœ¼ë ¤ë©´ TF-IDFì™€ ê°™ì€ ê°€ì¤‘ì¹˜ ê¸°ë²•ì„ ì¶”ê°€ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")