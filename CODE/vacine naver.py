from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
import requests, json, time, pandas as pd
from urllib.parse import quote
from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException

# ----------------------------
# 1. ë¸Œë¼ìš°ì € ì„¤ì • ë° ë“œë¼ì´ë²„ ì´ˆê¸°í™”
# ----------------------------
chrome_options = Options()
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument(
    "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

try:
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
except Exception as e:
    print(f"[ERROR] ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    exit()

# ----------------------------
# 2. ê²€ìƒ‰ì–´ ë° í˜ì´ì§€ ì„¤ì •
# ----------------------------
query = "ì½”ë¡œë‚˜ ë°±ì‹ "
max_pages = 5
news_urls = []
url_set = set()

print(f"[INFO] '{query}' ê´€ë ¨ ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")

# ----------------------------
# 3. ë‰´ìŠ¤ ë§í¬ ìˆ˜ì§‘ (BeautifulSoup ì œê±°, ìˆœìˆ˜ Selenium ì‚¬ìš©)
# ----------------------------
for page in range(1, max_pages + 1):
    start = (page - 1) * 10 + 1
    query_encoded = quote(query)
    url = (
        f"https://search.naver.com/search.naver?where=news&query={query_encoded}"
        f"&sm=tab_pge&sort=0&photo=0&field=0&pd=3&ds=2020.12.01&de=2022.12.31&mynews=0&office_type=0"
        f"&start={start}"
    )

    driver.get(url)
    time.sleep(3)

    # ìˆœìˆ˜ Selenium: HTML ì†ŒìŠ¤ ëŒ€ì‹  ìš”ì†Œ ìì²´ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    # a.news_titì´ í™•ì‹¤í•˜ë¯€ë¡œ, ì´ ìš”ì†Œë¥¼ ì§ì ‘ ì°¾ìŠµë‹ˆë‹¤.
    try:
        links = driver.find_elements(By.CSS_SELECTOR, "a.news_tit")
    except:
        links = []

    for link in links:
        href = link.get_attribute("href")  # Seleniumì—ì„œ ë°”ë¡œ href ì†ì„± ì¶”ì¶œ
        # ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬ë§Œ í•„í„°ë§í•˜ê³  ì¤‘ë³µ ë°©ì§€
        if href and "news.naver.com" in href and href not in url_set:
            news_urls.append(href)
            url_set.add(href)

    print(f"  > {page}í˜ì´ì§€ ì™„ë£Œ ({len(news_urls)}ê°œ ëˆ„ì )")
    time.sleep(1)

driver.quit()

# ----------------------------
# 4. CSV ì €ì¥ ë° ì¶œë ¥ (ë§í¬ ìˆ˜ì§‘ ì„±ê³µ ì‹œ ë‹¤ìŒ ë‹¨ê³„ì¸ ëŒ“ê¸€ í¬ë¡¤ë§ìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.)
# ----------------------------
print(f"\n[INFO] ì´ {len(news_urls)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ URL ìˆ˜ì§‘ ì™„ë£Œ.")

# ë§í¬ ìˆ˜ì§‘ì— ì„±ê³µí–ˆì„ ë•Œë§Œ ë‹¤ìŒ ëŒ“ê¸€ í¬ë¡¤ë§ ë‹¨ê³„ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.
if len(news_urls) > 0:
    # 5. ëŒ“ê¸€ API ê¸°ë°˜ ëŒ“ê¸€ í¬ë¡¤ë§ (ì´ ë¶€ë¶„ì€ ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì§„í–‰)
    # ...

    df = pd.DataFrame(news_urls, columns=['url'])
    output = "naver_vaccine_urls_final_success.csv"
    df.to_csv(output, index=False, encoding="utf-8-sig")
    print(f"ğŸ’¾ ë§í¬ ìˆ˜ì§‘ ì„±ê³µ. CSV ì €ì¥ ì™„ë£Œ: {output} (ë§í¬ ê°œìˆ˜: {len(df)})")
else:
    print("ğŸš¨ ë§í¬ ìˆ˜ì§‘ ì‹¤íŒ¨. ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰ ë¶ˆê°€.")