    import os
    import pandas as pd
    import torch
    from torch.utils.data import Dataset, DataLoader
    from torch.optim import AdamW
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, f1_score # F1-Score ì¶”ê°€
    from transformers import (
        ElectraTokenizer,
        ElectraForSequenceClassification,
        get_linear_schedule_with_warmup,
    )
    from tqdm import tqdm
    import numpy as np


    # ======================
    # Dataset class
    # ======================
    class KoDataset(Dataset):
        def __init__(self, texts, labels, tokenizer, max_len=128):
            self.texts = texts
            self.labels = labels
            self.tokenizer = tokenizer
            self.max_len = max_len

        def __len__(self):
            return len(self.texts)

        def __getitem__(self, idx):
            text = str(self.texts[idx])
            label = int(self.labels[idx])

            enc = self.tokenizer(
                text,
                truncation=True,
                padding='max_length',
                max_length=self.max_len,
                return_tensors="pt"
            )

            return {
                "input_ids": enc["input_ids"].squeeze(),
                "attention_mask": enc["attention_mask"].squeeze(),
                "label": torch.tensor(label, dtype=torch.long)
            }


    # ======================
    # Train (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©)
    # ======================
    def train_epoch(model, loader, optimizer, scheduler, device, class_weights=None):
        model.train()
        total_loss = 0

        # ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ (CrossEntropyLoss ì‚¬ìš©)
        if class_weights is not None:
            loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(device))
        else:
            loss_fn = torch.nn.CrossEntropyLoss()

        for batch in tqdm(loader, desc="Training", ncols=80):
            optimizer.zero_grad()

            input_ids = batch["input_ids"].to(device)
            attn_mask = batch["attention_mask"].to(device)
            labels = batch["label"].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attn_mask
            )

            loss = loss_fn(outputs.logits, labels)

            loss.backward()
            optimizer.step()
            scheduler.step()

            total_loss += loss.item()

        return total_loss / len(loader)


    # ======================
    # Eval (Val Loss, Acc, F1-Score ê³„ì‚°)
    # ======================
    def eval_epoch(model, loader, device, class_weights=None):
        model.eval()
        preds, trues = [], []
        total_loss = 0

        # ì†ì‹¤ í•¨ìˆ˜ ì •ì˜
        if class_weights is not None:
            loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(device))
        else:
            loss_fn = torch.nn.CrossEntropyLoss()

        with torch.no_grad():
            for batch in tqdm(loader, desc="Validating", ncols=80):
                input_ids = batch["input_ids"].to(device)
                attn_mask = batch["attention_mask"].to(device)
                labels = batch["label"].to(device)

                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attn_mask
                )

                logit = outputs.logits

                # Val Loss ê³„ì‚°
                loss = loss_fn(logit, labels)
                total_loss += loss.item()

                pred = torch.argmax(logit, dim=1).cpu().numpy()

                preds.extend(pred)
                trues.extend(batch["label"].numpy())

        avg_loss = total_loss / len(loader)
        acc = accuracy_score(trues, preds)
        # F1-Score ê³„ì‚° (ë¶ˆê· í˜• ë°ì´í„°ì—ì„œ í•µì‹¬ ì§€í‘œ)
        f1 = f1_score(trues, preds, average='binary', zero_division=0)

        return acc, avg_loss, f1


    # ======================
    # Main (ìµœì í™” ì„¤ì • ì ìš©)
    # ======================
    def main():
        # â˜… CSV íŒŒì¼ëª… ìë™ ê³ ì • â˜…
        csv_file = "labeled_output#.csv"

        base_dir = os.path.dirname(os.path.abspath(__file__))
        csv_path = os.path.join(base_dir, csv_file)

        print(f"ğŸ“„ Loading CSV File â†’ {csv_path}")

        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"CSV íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {csv_path}")

        df = pd.read_csv(csv_path)

        # ===== ì½ëŠ” ì»¬ëŸ¼ ìˆ˜ì •ë¨ =====
        texts = df["text"].astype(str).tolist()
        labels = df["sentiment"].astype(int).tolist()

        # ===== Split =====
        train_texts, val_texts, train_labels, val_labels = train_test_split(
            texts, labels, test_size=0.2, random_state=42
        )

        # ğŸŒŸ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ë¶ˆê· í˜• í•´ê²°)
        train_labels_array = np.array(train_labels)
        class_counts = np.bincount(train_labels_array)
        num_classes = len(class_counts)

        # ê°€ì¤‘ì¹˜ ê³„ì‚°
        total_samples = len(train_labels_array)
        small_epsilon = 1e-6
        class_weights = total_samples / (num_classes * (class_counts + small_epsilon))
        class_weights = torch.tensor(class_weights, dtype=torch.float)

        print(f"\nğŸ“Š Class Counts: {class_counts}")
        print(f"âš–ï¸ Calculated Class Weights (for loss function): {class_weights.tolist()}")


        # Device
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print("Device:", device)

        # Model/Tokenizer ë¡œë”©
        model_name = "monologg/koelectra-base-v3-discriminator"

        tokenizer = ElectraTokenizer.from_pretrained(model_name)
        model = ElectraForSequenceClassification.from_pretrained(
            model_name,
            num_labels=2,
            use_safetensors=True
        ).to(device)

        # Dataset/Loader ì •ì˜
        train_dataset = KoDataset(train_texts, train_labels, tokenizer)
        val_dataset = KoDataset(val_texts, val_labels, tokenizer)

        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)

        # Optimizer (ì•ˆì •ì ì¸ í•™ìŠµë¥  2e-5 ì ìš©)
        optimizer = AdamW(model.parameters(), lr=2e-5) # ğŸ’¡ í•™ìŠµë¥  2e-5 ì ìš©

        # Scheduler
        num_epochs = 3 # ğŸ’¡ ì—í¬í¬ 3íšŒë¡œ ì¤„ì—¬ ê³¼ì í•© ë°©ì§€
        total_steps = len(train_loader) * num_epochs
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=0,
            num_training_steps=total_steps
        )

        # Training
        print(f"\nğŸš€ Start Training for {num_epochs} Epochs...")
        for epoch in range(num_epochs):
            print(f"\n===== Epoch {epoch+1} / {num_epochs} =====")

            train_loss = train_epoch(model, train_loader, optimizer, scheduler, device, class_weights)
            val_acc, val_loss, val_f1 = eval_epoch(model, val_loader, device, class_weights) # F1-Score ë°˜í™˜

            print(f"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Acc: {val_acc:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f}") # F1-Score ì¶œë ¥

        print("\nğŸ‰ Training Completed!")


    if __name__ == "__main__":
        main()